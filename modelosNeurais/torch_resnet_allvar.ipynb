{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "import os\n",
    "\n",
    "from kornia.losses import *\n",
    "\n",
    "# Implementacao manual\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Treinar o modelo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcao Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FocalLoss2(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss2, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparacao dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('tabelacompleta.csv')\n",
    "df.drop(['url_img_satelite', 'img'], axis=1, inplace=True)\n",
    "\n",
    "# Transformando aeroporto em variaveis numericas\n",
    "enc = LabelEncoder()\n",
    "enc.fit(df['origem'].unique().tolist())\n",
    "df['origem'] = enc.transform(df['origem'])\n",
    "df['destino'] = enc.transform(df['destino'])\n",
    "\n",
    "# Scaler das variaveis\n",
    "scl = StandardScaler()\n",
    "df_aux = pd.DataFrame(scl.fit_transform(df.drop(['flightid', 'espera'], axis=1)), columns=df.drop(['flightid', 'espera'], axis=1).columns)\n",
    "df_aux['flightid'] = df['flightid']\n",
    "df_aux['espera'] = df['espera']\n",
    "\n",
    "# Separacao treino e teste kaggle\n",
    "df_train = df[df['espera'].notna()]\n",
    "df_test = df[df['espera'].isna()]\n",
    "\n",
    "# Removendo casos que nao foi possivel extrair imagens\n",
    "import glob\n",
    "imgs_validas = pd.Series(glob.glob('dados/imgs/ImagensHibrido/*')).apply(lambda txt: str(txt).split('\\\\')[-1].split('.')[0])\n",
    "df_train = df_train.loc[df_train['flightid'].isin(imgs_validas)]\n",
    "\n",
    "\n",
    "# Oversampling dos dados minoritarios\n",
    "oversampling = df_train.loc[df_train['espera']==1]\n",
    "n_oversamipling = 20\n",
    "\n",
    "for i in range(n_oversamipling):\n",
    "    df_train = pd.concat([df_train, oversampling], axis=0)\n",
    "\n",
    "train, val = train_test_split(df_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning final com proporcao 50:50\n",
    "df_fine_train = df[df['espera'].notna()]\n",
    "df_fine_train = df_fine_train.loc[df_fine_train['flightid'].isin(imgs_validas)]\n",
    "\n",
    "df_fine_train.loc[df_fine_train['espera'] == 0]\n",
    "\n",
    "espera = df_fine_train.loc[df_fine_train['espera']==1]\n",
    "n_oversamipling = 20\n",
    "\n",
    "sem_espera = df_fine_train.loc[df_fine_train['espera'] == 0]\n",
    "sem_espera, _ = train_test_split(sem_espera, train_size=espera.shape[0]*n_oversamipling/sem_espera.shape[0], random_state=42)\n",
    "\n",
    "for i in range(n_oversamipling):\n",
    "    sem_espera = pd.concat([sem_espera, oversampling], axis=0)\n",
    "\n",
    "fine_train, fine_val = train_test_split(sem_espera, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 23\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, num_classes, num_numeric_features):\n",
    "        super(HybridModel, self).__init__()\n",
    "\n",
    "        # ResNet para processamento de imagens\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Cauda densa da resnet\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),  \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Camadas para processamento de variáveis numéricas\n",
    "        self.numeric_layers = nn.Sequential(\n",
    "            nn.Linear(num_numeric_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # Camada final para combinar as saídas da ResNet e das camadas numéricas\n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(32, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, numeric):\n",
    "        image_features = self.resnet(image)\n",
    "        numeric_features = self.numeric_layers(numeric)\n",
    "        \n",
    "        # Combinando as características das imagens e numéricas\n",
    "        combined_features = torch.cat((image_features, numeric_features), dim=1)\n",
    "        \n",
    "        # Passando as características combinadas pelas camadas finais\n",
    "        output = self.final_layers(combined_features)\n",
    "        return output\n",
    "\n",
    "# Variaveis de entrada\n",
    "num_classes = df_train['espera'].nunique()  # Número de classes de saída\n",
    "num_numeric_features = df_train.drop(['espera', 'flightid'], axis=1).shape[1]  # Número de características numéricas\n",
    "\n",
    "# Criando uma instância do modelo\n",
    "model = HybridModel(num_classes, num_numeric_features)\n",
    "print(num_classes, num_numeric_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Iteravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import PIL\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),  # Redimensionar todas as imagens para 224x224\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, PATH, predict = False, image_transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_transform = image_transform\n",
    "        self.PATH = PATH\n",
    "        self.predict = predict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Consumir imagem\n",
    "        image = PIL.Image.open(self.PATH + row['flightid'] + '.jpg')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        # Dados numericos\n",
    "        numeric_features = row.drop(['espera', 'flightid']).values.astype(float)\n",
    "        \n",
    "        # Label \n",
    "        label = row['espera']\n",
    "\n",
    "        # Saida para treino ou predicao\n",
    "        if self.predict:\n",
    "            return image, numeric_features, label, row['flightid']\n",
    "        return image, numeric_features, label\n",
    "\n",
    "# Instância do conjunto de dados personalizado\n",
    "train_dataset = CustomDataset(dataframe=train, image_transform=data_transforms, PATH='dados/imgs/ImagensHibrido/')\n",
    "val_dataset = CustomDataset(dataframe=val, image_transform=data_transforms, PATH='dados/imgs/ImagensHibrido/')\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=True),\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_dataset),\n",
    "    'val': len(val_dataset)\n",
    "}\n",
    "\n",
    "# Definicao do conjunto de fine tuning\n",
    "fine_train_dataset = CustomDataset(dataframe=fine_train, image_transform=data_transforms, PATH='dados/imgs/ImagensHibrido/')\n",
    "fine_val_dataset = CustomDataset(dataframe=fine_val, image_transform=data_transforms, PATH='dados/imgs/ImagensHibrido/')\n",
    "\n",
    "fine_batch_size = 24\n",
    "fine_dataloaders = {\n",
    "    'train': DataLoader(fine_train_dataset, batch_size=batch_size, shuffle=True),\n",
    "    'val': DataLoader(fine_val_dataset, batch_size=batch_size, shuffle=True),\n",
    "}\n",
    "\n",
    "fine_dataset_sizes = {\n",
    "    'train': len(fine_train_dataset),\n",
    "    'val': len(fine_val_dataset)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1839/1839 [03:31<00:00,  8.69it/s, loss=0.418, accuracy=0.903, f1=0.903, auc=0.914, size=117696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4180 Acc: 0.9029 F1: 0.9028 AUC: 0.9144 Size: 117664.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 460/460 [00:49<00:00,  9.24it/s, loss=0.405, accuracy=0.911, f1=0.911, auc=0.916, size=29440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4051 Acc: 0.9119 F1: 0.9118 AUC: 0.9163 Size: 29416.0000\n",
      "Epoch 1/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1839/1839 [03:29<00:00,  8.77it/s, loss=0.418, accuracy=0.903, f1=0.902, auc=0.914, size=117696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4181 Acc: 0.9028 F1: 0.9027 AUC: 0.9146 Size: 117664.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 460/460 [00:49<00:00,  9.26it/s, loss=0.406, accuracy=0.911, f1=0.911, auc=0.917, size=29440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4064 Acc: 0.9119 F1: 0.9119 AUC: 0.9176 Size: 29416.0000\n",
      "Epoch 2/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1839/1839 [03:28<00:00,  8.83it/s, loss=0.418, accuracy=0.903, f1=0.903, auc=0.915, size=117696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4176 Acc: 0.9032 F1: 0.9031 AUC: 0.9148 Size: 117664.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 460/460 [00:49<00:00,  9.25it/s, loss=0.404, accuracy=0.913, f1=0.913, auc=0.915, size=29440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4047 Acc: 0.9142 F1: 0.9141 AUC: 0.9155 Size: 29416.0000\n",
      "Epoch 3/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1839/1839 [03:28<00:00,  8.83it/s, loss=0.417, accuracy=0.903, f1=0.903, auc=0.915, size=117696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4174 Acc: 0.9035 F1: 0.9034 AUC: 0.9149 Size: 117664.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 460/460 [00:49<00:00,  9.30it/s, loss=0.402, accuracy=0.915, f1=0.915, auc=0.916, size=29440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4019 Acc: 0.9160 F1: 0.9160 AUC: 0.9164 Size: 29416.0000\n",
      "Epoch 4/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1839/1839 [1:09:30<00:00,  2.27s/it, loss=0.417, accuracy=0.903, f1=0.903, auc=0.915, size=117696]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4170 Acc: 0.9035 F1: 0.9035 AUC: 0.9154 Size: 117664.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 460/460 [01:56<00:00,  3.93it/s, loss=0.403, accuracy=0.914, f1=0.914, auc=0.916, size=29440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4031 Acc: 0.9144 F1: 0.9144 AUC: 0.9169 Size: 29416.0000\n",
      "Epoch 5/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1839/1839 [05:29<00:00,  5.59it/s, loss=0.416, accuracy=0.904, f1=0.904, auc=0.915, size=117696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4165 Acc: 0.9044 F1: 0.9043 AUC: 0.9151 Size: 117664.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 460/460 [00:51<00:00,  8.85it/s, loss=0.403, accuracy=0.914, f1=0.914, auc=0.916, size=29440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4032 Acc: 0.9143 F1: 0.9143 AUC: 0.9169 Size: 29416.0000\n",
      "Epoch 6/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1839/1839 [03:42<00:00,  8.27it/s, loss=0.417, accuracy=0.904, f1=0.904, auc=0.915, size=117696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4167 Acc: 0.9042 F1: 0.9041 AUC: 0.9152 Size: 117664.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 460/460 [00:53<00:00,  8.54it/s, loss=0.406, accuracy=0.912, f1=0.912, auc=0.917, size=29440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4059 Acc: 0.9124 F1: 0.9124 AUC: 0.9173 Size: 29416.0000\n",
      "Epoch 7/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1839/1839 [03:34<00:00,  8.57it/s, loss=0.416, accuracy=0.905, f1=0.905, auc=0.915, size=117696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4159 Acc: 0.9051 F1: 0.9050 AUC: 0.9154 Size: 117664.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 460/460 [00:50<00:00,  9.03it/s, loss=0.403, accuracy=0.914, f1=0.914, auc=0.916, size=29440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4031 Acc: 0.9148 F1: 0.9148 AUC: 0.9166 Size: 29416.0000\n",
      "Epoch 8/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1839/1839 [03:37<00:00,  8.47it/s, loss=0.417, accuracy=0.904, f1=0.904, auc=0.915, size=117696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4168 Acc: 0.9043 F1: 0.9042 AUC: 0.9153 Size: 117664.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 460/460 [00:50<00:00,  9.15it/s, loss=0.403, accuracy=0.914, f1=0.914, auc=0.916, size=29440]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping! No improvement in validation loss for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "# weights = [0.4, 0.6]\n",
    "# class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# criterion = FocalLoss2()\n",
    "# criterion = FocalLoss2(alpha = 0.7, gamma=1)\n",
    "\n",
    "parametros = list(model.resnet.fc.parameters()) + list(model.numeric_layers.parameters()) + list(model.final_layers.parameters())\n",
    "\n",
    "# optimizer = optim.Adam(parametros, lr=0.0005, weight_decay=0.001)\n",
    "optimizer = optim.SGD(parametros, lr=0.0005, weight_decay=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "def calculate_f1(preds, labels):\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    return f1_score(labels, preds, average='weighted')\n",
    "\n",
    "def safe_auc_score(y_true, y_score):\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_score)\n",
    "    except ValueError:\n",
    "        auc = 0.5  # AUC é 0.5 quando todos os rótulos pertencem a uma única classe\n",
    "    return auc\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, scheduler=None, num_epochs=25, patience = 15):\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Fase de treinamento\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        running_f1 = 0.0\n",
    "        running_auc = 0.0\n",
    "        progress_bar = tqdm(dataloaders['train'], desc='Training')\n",
    "        iteration = 1\n",
    "        for img_inputs, num_inputs, labels in progress_bar:\n",
    "            img_inputs = img_inputs.to(device, dtype=torch.float32)\n",
    "            num_inputs = num_inputs.to(device, dtype=torch.float32)\n",
    "\n",
    "            outputs = model(img_inputs, num_inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            labels = labels.to(device, dtype=torch.int64) \n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.retain_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            f1 = calculate_f1(preds, labels)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            auc = safe_auc_score(labels.cpu().numpy(), probs.detach().cpu().numpy()[:, 1])\n",
    "\n",
    "\n",
    "            running_loss += loss.item() * num_inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            running_f1 += f1 * num_inputs.size(0)\n",
    "            running_auc += auc * num_inputs.size(0)\n",
    "\n",
    "            # Atualizar a barra de progresso\n",
    "            mean_it = iteration*batch_size\n",
    "            progress_bar.set_postfix({'loss': running_loss/mean_it, \n",
    "                                      'accuracy': float(running_corrects.double()/mean_it),\n",
    "                                      'f1': running_f1/mean_it,\n",
    "                                      'auc': running_auc/mean_it,\n",
    "                                      'size': mean_it})\n",
    "            iteration += 1\n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / dataset_sizes['train']\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes['train']\n",
    "        epoch_f1 = running_f1 / dataset_sizes['train']\n",
    "        epoch_auc = running_auc / dataset_sizes['train']\n",
    "\n",
    "        print('Train Loss: {:.4f} Acc: {:.4f} F1: {:.4f} AUC: {:.4f} Size: {:.4f}'.format(epoch_loss, epoch_acc, epoch_f1, epoch_auc, dataset_sizes['train']))\n",
    "\n",
    "        # Fase de validação\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        running_f1 = 0.0\n",
    "        running_auc = 0.0\n",
    "        progress_bar = tqdm(dataloaders['val'], desc='Validation')\n",
    "        iteration = 1\n",
    "        for img_inputs, num_inputs, labels in progress_bar:\n",
    "            img_inputs = img_inputs.to(device, dtype=torch.float32)\n",
    "            num_inputs = num_inputs.to(device, dtype=torch.float32)  \n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(img_inputs, num_inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                labels = labels.to(device, dtype=torch.int64) \n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                f1 = calculate_f1(preds, labels)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                auc = safe_auc_score(labels.cpu().numpy(), probs.detach().cpu().numpy()[:, 1])\n",
    "\n",
    "            running_loss += loss.item() * num_inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            running_f1 += f1 * num_inputs.size(0)\n",
    "            running_auc += auc * num_inputs.size(0)\n",
    "\n",
    "            # Atualizar a barra de progresso\n",
    "            mean_it = iteration*batch_size\n",
    "            progress_bar.set_postfix({'loss': running_loss/mean_it, \n",
    "                                      'accuracy': float(running_corrects.double()/mean_it),\n",
    "                                      'f1': running_f1/mean_it,\n",
    "                                      'auc': running_auc/mean_it,\n",
    "                                      'size': mean_it})\n",
    "            iteration += 1\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_sizes['val']\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes['val']\n",
    "        epoch_f1 = running_f1 / dataset_sizes['val']\n",
    "        epoch_auc = running_auc / dataset_sizes['val']\n",
    "\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Salvar os pesos do modelo\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Verificar se o treinamento deve ser interrompido\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping! No improvement in validation loss for {} epochs.\".format(patience))\n",
    "            break\n",
    "\n",
    "\n",
    "        print('Val Loss: {:.4f} Acc: {:.4f} F1: {:.4f} AUC: {:.4f} Size: {:.4f}'.format(epoch_loss, epoch_acc, epoch_f1, epoch_auc, dataset_sizes['val']))\n",
    "\n",
    "train_model(model, criterion, optimizer, fine_dataloaders, fine_dataset_sizes, scheduler, num_epochs=200, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'D:/GitHub/DesafioITA/modelos/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('D:/GitHub/DesafioITA/modelos/best_model.pth'))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flightid</th>\n",
       "      <th>espera</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45e7978b9d88f934cc06c11b6f0edba7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16ed22b3755aa9196d16fdd2a173c98f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b548d2c700496e2536d78caf626aee17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e4cc2545104bcfe978912d39f0960f4e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ace87fdae884359186e9851c38b146fb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90715</th>\n",
       "      <td>8ca8c4d16e592d65cd25d341113aecdf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90716</th>\n",
       "      <td>afde50d413c2374ab53cd3101332f9fc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90717</th>\n",
       "      <td>faeb2f6e744c606aee9e819ea1d62e6b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90718</th>\n",
       "      <td>3992d063be713b35d3bf46b79853880e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90719</th>\n",
       "      <td>b9978f57bd67e255fb55f3299e39490e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               flightid  espera\n",
       "0      45e7978b9d88f934cc06c11b6f0edba7       0\n",
       "1      16ed22b3755aa9196d16fdd2a173c98f       0\n",
       "2      b548d2c700496e2536d78caf626aee17       0\n",
       "3      e4cc2545104bcfe978912d39f0960f4e       0\n",
       "4      ace87fdae884359186e9851c38b146fb       0\n",
       "...                                 ...     ...\n",
       "90715  8ca8c4d16e592d65cd25d341113aecdf       0\n",
       "90716  afde50d413c2374ab53cd3101332f9fc       0\n",
       "90717  faeb2f6e744c606aee9e819ea1d62e6b       0\n",
       "90718  3992d063be713b35d3bf46b79853880e       0\n",
       "90719  b9978f57bd67e255fb55f3299e39490e       0\n",
       "\n",
       "[90720 rows x 2 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "df_preds = df_train.loc[df_train['espera'] == 1].drop_duplicates()\n",
    "path = 'dados/imgs/ImagensHibrido/'\n",
    "df_preds = df_test.drop_duplicates()\n",
    "path = 'dados/test/'\n",
    "dataset = CustomDataset(dataframe=df_preds, image_transform=data_transforms, PATH=path, predict=True)\n",
    "predicoes = []\n",
    "flightid = []\n",
    "for img, numvar, label, id in DataLoader(dataset, batch_size=1):\n",
    "    img = img.to(device, dtype=torch.float32) \n",
    "    numvar = numvar.to(device, dtype=torch.float32)   \n",
    "    outputs = model(img, numvar)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    preds = preds.to('cpu')\n",
    "    predicoes.append(preds.numpy()[0])\n",
    "    flightid.append(id[0])\n",
    "preds = pd.DataFrame()\n",
    "preds['flightid'] = flightid\n",
    "preds['espera'] = predicoes\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_csv('resnet_numvar_bestmodel_91AUC.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
